{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ceabfe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.signal import savgol_filter\n",
    "import numpy as np\n",
    "import os \n",
    "import re\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "45b607d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_exp_labels(df, exp):\n",
    "    df[\"exp\"] = [exp] * len(df)\n",
    "\n",
    "    exp_labels = {\n",
    "        \"M4576_s2\": \"10mM Pulses #3\",\n",
    "        \"M4581_s1\": \"10mM Pulses #1\",\n",
    "        \"M4584_s1\": \"10mM Pulses #2\",\n",
    "        \"M6881_s2\": \"1mM Pulses #1\",\n",
    "        \"M6881_s5\": \"0.1mM Pulses #1\",\n",
    "        \"M6881_s6\": \"0.01mM Pulses #1\",\n",
    "        # autotht\n",
    "        \"M6813_s1\": \"0.01mM Cont. #1\",\n",
    "        \"M6813_s4\": \"0.01mM Cont. #2\",\n",
    "        \"M6605_s3\": \"10mM Pulses #1\",\n",
    "        \"M6605_s4\": \"10mM Pulses #2\",\n",
    "        \"M6605_s8\": \"10mM Pulses #3\"\n",
    "    }\n",
    "\n",
    "    df[\"Exp_Label\"] = df[\"exp\"].map(exp_labels)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "aba15c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_delta_intensity(df):\n",
    "  df = df.copy()\n",
    "  df[\"Delta_ThT\"] = df[\"Intensity_ThT\"] - df[\"Initial_Intensity_ThT\"]\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cc2e7f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_styling(df, column, cmap_name=\"tab20\", num_samples=20):\n",
    "    unique_vals = df[column].unique()\n",
    "    unique_vals = list(unique_vals)[:num_samples]\n",
    "    cmap = plt.get_cmap(cmap_name, len(unique_vals))\n",
    "    color_dict = {val: cmap(i) for i, val in enumerate(unique_vals)}\n",
    "\n",
    "    return color_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a46d0e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(df):\n",
    "    # This pattern now correctly handles column names like 'Mean1.0' and 'Perim.1.0'\n",
    "    # It looks for (text_with_dots)(numbers)(.some_other_numbers_at_the_end)\n",
    "    pattern = re.compile(r\"([A-Za-z_.]+)(\\d+)\\.\\d+$\")\n",
    "\n",
    "    feature_map = {}\n",
    "    for col in df.columns:\n",
    "        match = pattern.match(col)\n",
    "        if match:\n",
    "            prefix, idx = match.groups()\n",
    "            # Clean up the prefix by removing any trailing dot (like from 'Perim.')\n",
    "            prefix = prefix.strip('.')\n",
    "            if idx not in feature_map:\n",
    "                feature_map[idx] = {}\n",
    "            feature_map[idx][prefix] = col\n",
    "\n",
    "    dfs = []\n",
    "\n",
    "    if not feature_map:\n",
    "        print(\"WARNING: extract_features did not find any matching columns.\")\n",
    "        print(\"Please check the regex pattern against your CSV's column headers.\")\n",
    "        return pd.DataFrame() # Return an empty dataframe to avoid crashing\n",
    "\n",
    "    for idx, feature_cols in feature_map.items():\n",
    "        sub_cols = list(feature_cols.values())\n",
    "        sub_df = df[sub_cols].copy()\n",
    "\n",
    "        rename_dict = {v: k for k, v in feature_cols.items()}\n",
    "        sub_df.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "        for col in [\"Mean\", \"X\", \"Y\"]:\n",
    "            if col not in sub_df.columns:\n",
    "                sub_df[col] = None\n",
    "\n",
    "        sub_df[\"Track\"] = idx\n",
    "        sub_df[\"Frame\"] = range(len(df)) \n",
    "\n",
    "        dfs.append(sub_df)\n",
    "\n",
    "    df_long = pd.concat(dfs, ignore_index=True)\n",
    "    df_long.rename(columns={\"Mean\": \"Intensity\"}, inplace=True)\n",
    "\n",
    "    return df_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b2f2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "def add_germination_time(df, feature = \"DerivSavgol_Intensity\", threshold=-8):\n",
    "    spore_data = df.groupby(\"Track_ID\")\n",
    "    phase_df = []\n",
    "    skip_ids = []\n",
    "    for spore_id, spore in spore_data:\n",
    "        spore = spore.sort_values(\"Frame\").copy()\n",
    "\n",
    "        drop_frames = spore[spore[feature] < threshold]\n",
    "\n",
    "        germ_frame = drop_frames[\"Frame\"].min() if not drop_frames.empty else None\n",
    "        spore[\"Germination_Index\"] = [germ_frame] * len(spore)\n",
    "        # assign germination status\n",
    "        if germ_frame is not None:\n",
    "            spore[\"Status\"] = (spore[\"Frame\"] >= germ_frame).astype(int)\n",
    "        else:\n",
    "            skip_ids.append(spore_id)\n",
    "            spore[\"Status\"] = 0\n",
    "\n",
    "        phase_df.append(spore)\n",
    "    df = pd.concat(phase_df, ignore_index=True)\n",
    "    return df[~df[\"Track_ID\"].isin(skip_ids)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0b7aea21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_derivative_savgol(df, feature, track_column, window_length=7, poly_order=3):\n",
    "    df = df.copy()\n",
    "    deriv_col = f\"DerivSavgol_Intensity\"\n",
    "    results = []\n",
    "\n",
    "    spore_data = df.groupby([track_column])\n",
    "    for spore_id, spore in spore_data:\n",
    "        spore = spore.sort_values(\"Frame\").copy()\n",
    "        x = spore[\"Frame\"].values\n",
    "        y = spore[feature].values.astype(float)\n",
    "\n",
    "\n",
    "        dt = np.median(np.diff(x))\n",
    "        dy_dx = savgol_filter(y, window_length, poly_order, deriv=1, delta=dt)\n",
    "\n",
    "        spore[deriv_col] = dy_dx\n",
    "        results.append(spore)\n",
    "\n",
    "    return pd.concat(results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "53f07a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_xy(df):\n",
    "  pos_dict = {}\n",
    "  spore_data = df.groupby([\"Track_ID\"])\n",
    "  for track, spore in spore_data:\n",
    "    avg_x = spore[\"X\"].mean()\n",
    "    avg_y = spore[\"Y\"].mean()\n",
    "    track_id = spore[\"Track_ID\"].values[0]\n",
    "    pos_dict[track_id] = (avg_x, avg_y)\n",
    "  return pos_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b91d11f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_ids(dict1, dict2, max_distance=5):\n",
    "    matches = []\n",
    "    used_tht_ids = set()  \n",
    "\n",
    "    for id1, (x1, y1) in dict1.items():\n",
    "        closest_id = None\n",
    "        min_dist = float('inf')\n",
    "\n",
    "        for id2, (x2, y2) in dict2.items():\n",
    "            if id2 in used_tht_ids:\n",
    "                continue  # skip if already matched\n",
    "\n",
    "            dist = math.sqrt((x1 - x2)**2 + (y1 - y2)**2)\n",
    "            if dist < min_dist and dist <= max_distance:\n",
    "                min_dist = dist\n",
    "                closest_id = id2\n",
    "\n",
    "        if closest_id is not None:\n",
    "            matches.append((id1, closest_id))\n",
    "            used_tht_ids.add(closest_id)\n",
    "\n",
    "    print(f\"matched {len(matches)} tracks...\")\n",
    "    return matches\n",
    "\n",
    "\n",
    "def merge_matched_tracks(phc_df, tht_df, matched_pairs, phc_suffix=\"PhC\", tht_suffix=\"ThT\"):\n",
    "    merged_list = []\n",
    "    for phc_id, tht_id in matched_pairs:\n",
    "        phc_track = phc_df[phc_df[\"Track\"] == phc_id].copy()\n",
    "        tht_track = tht_df[tht_df[\"Track\"] == tht_id].copy()\n",
    "\n",
    "        phc_track = phc_track.add_suffix(f\"_{phc_suffix}\")\n",
    "        tht_track = tht_track.add_suffix(f\"_{tht_suffix}\")\n",
    "\n",
    "        phc_track = phc_track.rename(columns={f\"Frame_{phc_suffix}\": \"Frame\"})\n",
    "        tht_track = tht_track.rename(columns={f\"Frame_{tht_suffix}\": \"Frame\"})\n",
    "\n",
    "        merged = pd.merge(phc_track, tht_track, on=\"Frame\", how=\"right\")\n",
    "        #merged[\"MATCHED_PAIR\"] = f\"{phc_id}_{tht_id}\"\n",
    "        merged_list.append(merged)\n",
    "\n",
    "    return pd.concat(merged_list, ignore_index=True) if merged_list else pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3b8686a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_interval_germ_exposures(initial_min, between_min, minutes_between_frames, exp_length):\n",
    "   i = initial_min/minutes_between_frames - 1\n",
    "   exposures = [i]\n",
    "   while i < exp_length:\n",
    "      i += between_min/minutes_between_frames \n",
    "      exposures.append(i)\n",
    "   if exposures[-1] < exp_length:\n",
    "      exposures = exposures[:-1]\n",
    "   return exposures\n",
    "def add_germ_exposures_todf(df, germ_exposures_list, concentration):\n",
    "    df[\"Germinant\"] = df[\"Frame\"].apply(lambda f: 1 if f in germ_exposures_list else 0)\n",
    "    df[\"Germinant\"] = df[\"Germinant\"].apply(lambda f: f * concentration)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e646bfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_intensity(df, color_dict, feature = \"Intensity_ThT\", alpha = 1, linestyle = \"-\", num_samples = None):\n",
    "  germ_col = \"Status_PhC\"\n",
    "  spore_data = df.groupby(\"Track_PhC\")\n",
    "  if num_samples != None:\n",
    "      samples = 0\n",
    "  for track_id, data in spore_data:\n",
    "      exp = data[\"exp\"].iloc[0]  # Add this\n",
    "      color = color_dict.get(track_id, \"gray\")\n",
    "\n",
    "      dormant_data = data[data[germ_col] == 0]\n",
    "      germinated_data = data[data[germ_col] == 1]\n",
    "\n",
    "      sns.lineplot(x=\"Frame\", y=feature, data=dormant_data,\n",
    "                     linewidth=2, color = color, alpha = alpha, label = feature.replace(\"_\", \" \"), linestyle = linestyle)\n",
    "        #sns.lineplot(x = \"FRAME\", y = feature, data = data, alpha = alpha, color = color)\n",
    "      if not germinated_data.empty:\n",
    "            sns.lineplot(x=\"Frame\", y=feature, data=germinated_data,\n",
    "                         linewidth=6, color = color, alpha = alpha, linestyle = linestyle)\n",
    "      if samples > num_samples - 1: \n",
    "          return \n",
    "      samples += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "47ab6f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def manually_filter_spores(base, df, feature, display_time=1, filter = 1):\n",
    "\n",
    "    # Grab experiment name safely\n",
    "    exp = df[\"exp\"].iloc[0] if \"exp\" in df.columns else \"unknown\"\n",
    "    \n",
    "    # Set up ignore file path\n",
    "    ignore_file = os.path.join(base, f\"{exp}_FilteredPhCTracks.csv\")\n",
    "\n",
    "    # Load ignored spores\n",
    "    if os.path.exists(ignore_file):\n",
    "        ignored_spores = set(map(str, pd.read_csv(ignore_file)[\"Track_PhC\"]))\n",
    "    else:\n",
    "        ignored_spores = set()\n",
    "\n",
    "    spores_to_keep = []\n",
    "    spores_to_ignore = []\n",
    "    if filter == 0: \n",
    "        print(\"skipping filtering...\")\n",
    "        print(f'keeping {df[\"Track_PhC\"][~df[\"Track_PhC\"].isin(ignored_spores)].nunique()} spores...')\n",
    "        return df[~df[\"Track_PhC\"].isin(ignored_spores)]\n",
    "    spore_data = df.groupby(\"Track_PhC\")\n",
    "    for spore_id, data in spore_data:\n",
    "\n",
    "        if spore_id in ignored_spores:\n",
    "            print(f\"found\")\n",
    "            continue\n",
    "\n",
    "        # Plot\n",
    "        fig, ax = plt.subplots(figsize=(3, 1))\n",
    "        sns.lineplot(data=data, x=\"Frame\", y=feature, ax=ax)\n",
    "        ax.axvline(data[\"Germination_Index_PhC\"].values[0], color='red', linestyle='--')\n",
    "        plt.show(block=False)\n",
    "        plt.pause(display_time)\n",
    "        plt.close(fig)\n",
    "\n",
    "        keep = input(f\"Keep spore {spore_id}? (y/n/a): \").strip().lower()\n",
    "        if keep not in [\"a\", \"y\", \"n\"]:\n",
    "            print(\"Invalid input... showing again...\")\n",
    "            keep = \"a\"\n",
    "\n",
    "        if keep == \"a\":\n",
    "            fig, ax = plt.subplots(figsize=(3, 1))\n",
    "            sns.lineplot(data=data, x=\"Frame\", y=feature, ax=ax)\n",
    "            ax.axvline(data[\"Germination_Index_PhC\"].values[0], color='red', linestyle='--')\n",
    "            plt.show(block=False)\n",
    "            plt.pause(display_time)\n",
    "            plt.close(fig)\n",
    "            keep = input(f\"Keep spore {spore_id}? (y/n): \").strip().lower()\n",
    "\n",
    "        if keep == \"y\":\n",
    "            spores_to_keep.append(spore_id)\n",
    "        elif keep == \"n\":\n",
    "            spores_to_ignore.append(spore_id)\n",
    "\n",
    "    # Save ignored spores\n",
    "    if spores_to_ignore:\n",
    "        new_ignored = pd.DataFrame(spores_to_ignore, columns=[\"Track_PhC\"])\n",
    "        if os.path.exists(ignore_file):\n",
    "            existing = pd.read_csv(ignore_file)\n",
    "            updated = pd.concat([existing, new_ignored], ignore_index=True).drop_duplicates()\n",
    "        else:\n",
    "            updated = new_ignored\n",
    "        updated.to_csv(ignore_file, index=False)\n",
    "\n",
    "    print(f\"Keeping {len(spores_to_keep)} spores...\")\n",
    "\n",
    "    # Return filtered DataFrame\n",
    "    return df[df[\"Track_PhC\"].isin(spores_to_keep)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a0f31997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rolling_derivative(df, feature, track_column, window=5):\n",
    "    df = df.copy()\n",
    "    deriv_col = f\"Derivative_{feature}\"\n",
    "    results = []\n",
    "\n",
    "    spore_data = df.groupby([\"Exp_Label\", track_column])\n",
    "    for (exp_label, spore_id), spore in spore_data:\n",
    "        spore = spore.sort_values(\"Frame\").copy()\n",
    "        rolling_diff = spore[feature].diff().rolling(window=window, min_periods=1).mean()\n",
    "        spore[deriv_col] = rolling_diff.fillna(0)\n",
    "        results.append(spore)\n",
    "\n",
    "    return pd.concat(results, ignore_index=True)\n",
    "def add_rolling_mean(df, feature, track_column, window=5):\n",
    "    df = df.copy()\n",
    "    smoothed_col = f\"Rolling_{feature}\"\n",
    "    result = []\n",
    "\n",
    "    spore_data = df.groupby([\"Exp_Label\", track_column])\n",
    "    for spore_id, spore in spore_data:\n",
    "        spore = spore.sort_values(\"Frame\").copy()\n",
    "        spore[smoothed_col] = spore[feature].rolling(window=window, min_periods=1).mean()\n",
    "        result.append(spore)\n",
    "\n",
    "    return pd.concat(result, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4962e879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_initial_feature(df, feature, track_column, num_init = 11):\n",
    "  df = df.copy()\n",
    "  df = df.sort_values(\"Frame\")\n",
    "\n",
    "  init_col_name = f\"Initial_{feature}\"\n",
    "  column_values = []\n",
    "\n",
    "  spore_data = df.groupby([\"Exp_Label\", track_column])\n",
    "  for spore_id, spore in spore_data:\n",
    "        feature_vals = spore[feature].values.astype(float)\n",
    "        initial_feature = np.mean(feature_vals[:num_init])\n",
    "\n",
    "        spore[init_col_name] = [initial_feature] * len(spore)\n",
    "        column_values.append(spore)\n",
    "\n",
    "  df_return = pd.concat(column_values, ignore_index = True)\n",
    "  return df_return \n",
    "\n",
    "def add_cumsum_feature(df, feature, track_column = \"Track_PhC\"):#df, feature, track_column=\"TRACK_ID\"):\n",
    "\n",
    "    df = df.sort_values([track_column, \"Frame\"]).copy()\n",
    "    cumsum_col = f\"CumuSum_{feature}\"\n",
    "    df[cumsum_col] = df.groupby(track_column)[feature].cumsum()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "26f03770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_phase(df, feature = \"DerivSavgol_Intensity_PhC\", frame_col = \"Frame\", intensity_col = \"Intensity_PhC\"):\n",
    "  for spore_id, spore_data in df.groupby(\"Track_PhC\"):\n",
    "    #plt.axhline(-8, color = \"lightgrey\", label = \"Threshold\")\n",
    "    germinated_df = spore_data[spore_data[\"Status_PhC\"] == 1]\n",
    "    dormant_df = spore_data[spore_data[\"Status_PhC\"] == 0]\n",
    "    sns.lineplot(x = frame_col, y = intensity_col, data = spore_data, color = \"tab:blue\", linewidth = 3, label = \"Phase Intensity\", alpha = 1)\n",
    "\n",
    "    sns.lineplot(x = frame_col, y = feature, data = spore_data, color = \"tab:orange\", linewidth = 3, label = f\"Savitzky-Golay Derivative\")\n",
    "    #sns.lineplot(x = frame_col, y = feature, data = spore_data, color = \"tab:orange\", linewidth = 1, alpha = 0.5)\n",
    "    #sns.lineplot(x = frame_col, y = feature, data = germinated_df, color =\"orange\", linewidth = 5)\n",
    "    plt.axvline(spore_data[\"Germination_Index_PhC\"].values[0], label = \"Threshold Met\", linewidth = 3, color = \"darkgrey\", alpha = 0.7)\n",
    "\n",
    "    #sns.lineplot(x = frame_col, y = intensity_col, data = germinated_df, color = \"tab:blue\", linewidth = 5, label = \"PhC: phase-dark\")\n",
    "    plt.xlabel(\"Frame\", fontsize = 14)\n",
    "    plt.ylabel(\"\")\n",
    "    plt.xticks(fontsize = 10)\n",
    "    plt.yticks(fontsize = 10)\n",
    "    plt.legend(fontsize = 12)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7997863d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for results files in: /Users/mratcliff/Documents/GitHub/AroProject/data/processed/Leticia_M4576_s2_Fiji\n",
      "Skipping non-results file: .DS_Store\n",
      "-> Found 'ThT' data, loading...\n",
      "Skipping non-results file: Processed_Data\n",
      "-> Found 'PHC' data, loading...\n",
      "\n",
      "SUCCESS: Both PhC and ThT data found. Merging and analyzing...\n",
      "matched 1131 tracks...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Track'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/cellpose/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Track'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 68\u001b[0m\n\u001b[1;32m     65\u001b[0m matches \u001b[38;5;241m=\u001b[39m match_ids(phc_xy, tht_xy)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Merge the dataframes into a single master dataframe 'df'\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mmerge_matched_tracks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphc_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtht_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphc_suffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPhC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtht_suffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m df \u001b[38;5;241m=\u001b[39m add_exp_labels(df, EXPERIMENT)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# --- Feature Engineering and Analysis on Merged Data ---\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Process PhC data\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[91], line 29\u001b[0m, in \u001b[0;36mmerge_matched_tracks\u001b[0;34m(phc_df, tht_df, matched_pairs, phc_suffix, tht_suffix)\u001b[0m\n\u001b[1;32m     27\u001b[0m merged_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m phc_id, tht_id \u001b[38;5;129;01min\u001b[39;00m matched_pairs:\n\u001b[0;32m---> 29\u001b[0m     phc_track \u001b[38;5;241m=\u001b[39m phc_df[\u001b[43mphc_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrack\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m phc_id]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     30\u001b[0m     tht_track \u001b[38;5;241m=\u001b[39m tht_df[tht_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrack\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m tht_id]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     32\u001b[0m     phc_track \u001b[38;5;241m=\u001b[39m phc_track\u001b[38;5;241m.\u001b[39madd_suffix(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mphc_suffix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/cellpose/lib/python3.10/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/cellpose/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Track'"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "load_dotenv()\n",
    "# Load the main experiment config from the environment\n",
    "DATA_ROOT = os.getenv(\"DATA_ROOT\")\n",
    "EXPERIMENT = os.getenv(\"EXPERIMENT_NAME\")\n",
    "\n",
    "if not all([DATA_ROOT, EXPERIMENT]):\n",
    "    raise ValueError(\"DATA_ROOT or EXPERIMENT_NAME not set in your .env file.\")\n",
    "\n",
    "# This variable is for labeling the output file, you can change it as needed\n",
    "EXP_LABEL = \"Pulses_ThT_Analysis\" \n",
    "\n",
    "# --- Static variables for this script's specific purpose ---\n",
    "# This script's job is to merge PhC and a fluorescent channel.\n",
    "MICR_PHC = \"PH\"\n",
    "MICR_FLUOR = \"ThT\"\n",
    "\n",
    "BASE = Path(DATA_ROOT)\n",
    "\n",
    "# --- Path Setup ---\n",
    "# CORRECTED: Added \"data\" and \"processed\" to the path to match your structure\n",
    "fiji_base = BASE / \"data\" / \"processed\" / f\"{EXPERIMENT}_Fiji\"\n",
    "data_folder = fiji_base / \"Processed_Data\"\n",
    "\n",
    "# Create the output directory if it doesn't exist to prevent errors on save.\n",
    "data_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Data Loading and Initial Processing ---\n",
    "# Initialize dataframes as None. We will check later if they were successfully loaded.\n",
    "phc_df = None\n",
    "tht_df = None\n",
    "\n",
    "print(f\"Searching for results files in: {fiji_base}\")\n",
    "try:\n",
    "    for csv_filename in os.listdir(fiji_base):\n",
    "        if \"_Results.csv\" not in csv_filename:\n",
    "            print(f\"Skipping non-results file: {csv_filename}\")\n",
    "            continue  # Skip any file that isn't a results file\n",
    "\n",
    "        # Full path to the current CSV file\n",
    "        input_csv_path = os.path.join(fiji_base, csv_filename)\n",
    "        \n",
    "        if f\"_{MICR_PHC}_\" in csv_filename:\n",
    "            print(f\"-> Found 'PHC' data, loading...\")\n",
    "            phc_df = pd.read_csv(input_csv_path)\n",
    "        elif f\"_{MICR_FLUOR}_\" in csv_filename:\n",
    "            print(f\"-> Found 'ThT' data, loading...\")\n",
    "            tht_df = pd.read_csv(input_csv_path)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"---\")\n",
    "    print(f\"ERROR: The directory was not found.\")\n",
    "    print(f\"Please verify this path exists: {fiji_base}\")\n",
    "    phc_df, tht_df = None, None # Ensure dataframes are None so merge step is skipped\n",
    "\n",
    "\n",
    "# --- Conditional Merging and Analysis ---\n",
    "# This block only runs if BOTH phc_df and tht_df were successfully loaded in the step above.\n",
    "if phc_df is not None and tht_df is not None:\n",
    "    print(\"\\nSUCCESS: Both PhC and ThT data found. Merging and analyzing...\")\n",
    "\n",
    "    # Match spores between the two channels based on XY coordinates\n",
    "    phc_xy = extract_xy(phc_df)\n",
    "    tht_xy = extract_xy(tht_df)\n",
    "    matches = match_ids(phc_xy, tht_xy)\n",
    "    \n",
    "    # Merge the dataframes into a single master dataframe 'df'\n",
    "    df = merge_matched_tracks(phc_df, tht_df, matches, phc_suffix=\"PhC\", tht_suffix=\"ThT\")\n",
    "    df = add_exp_labels(df, EXPERIMENT)\n",
    "\n",
    "    # --- Feature Engineering and Analysis on Merged Data ---\n",
    "    # Process PhC data\n",
    "    df = add_derivative_savgol(df, \"Intensity_PhC\", \"Track_PhC\", window_length=7, poly_order=3)\n",
    "    df = add_germination_time(df, feature=\"DerivSavgol_Intensity_PhC\")\n",
    "    df = add_initial_feature(df, \"Intensity_PhC\", \"Track_PhC\")\n",
    "    \n",
    "    # Process ThT data\n",
    "    feature_tht = f\"Intensity_{MICR_FLUOR}\"\n",
    "    df = add_rolling_mean(df, feature_tht, \"Track_PhC\")\n",
    "    df = add_rolling_derivative(df, feature_tht, \"Track_PhC\")\n",
    "    df = add_initial_feature(df, feature_tht, \"Track_PhC\")\n",
    "    df = add_cumsum_feature(df, feature_tht, \"Track_PhC\")\n",
    "    df = add_delta_intensity(df)\n",
    "\n",
    "    # Plot the results\n",
    "    plot_phase(df)\n",
    "\n",
    "    # Save the final, merged, and processed data\n",
    "    final_output_path = os.path.join(data_folder, f\"{EXP}_Matched_And_Processed_Data.csv\")\n",
    "    df.to_csv(final_output_path, index=False)\n",
    "    print(f\"\\nANALYSIS COMPLETE. Final merged data saved to:\\n{final_output_path}\")\n",
    "\n",
    "else:\n",
    "    print(\"---\")\n",
    "    print(f\"SKIPPING MERGE STEP: Could not load both PhC and {MICR_FLUOR} data.\")\n",
    "    print(\"Please ensure both '_PhC_Results.csv' and '_ThT_Results.csv' files are present.\")\n",
    "    \n",
    "    if phc_df is not None:\n",
    "        print(\"However, PhC data was processed successfully.\")\n",
    "        \n",
    "        # You could add PhC-only analysis or plotting here if you wish.\n",
    "        \n",
    "        # Define an output path and save the processed PhC dataframe\n",
    "        phc_only_output_path = os.path.join(data_folder, f\"{EXP}_PhC_Processed_Data.csv\")\n",
    "        phc_df.to_csv(phc_only_output_path, index=False)\n",
    "        \n",
    "        print(f\"-> Saved processed PhC-only data to:\\n{phc_only_output_path}\")\n",
    "    else: \n",
    "        print(\"No PhC data found. Please check your input files.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cellpose",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
